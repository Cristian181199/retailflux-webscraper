# Dokploy Deployment Guide üöÄ

Esta gu√≠a te ayudar√° a desplegar RetailFlux WebScraper en Dokploy con scheduling autom√°tico.

## üìã Prerrequisitos

- ‚úÖ Dokploy configurado y funcionando
- ‚úÖ Base de datos PostgreSQL existente en Dokploy
- ‚úÖ Variables de entorno de base de datos configuradas a nivel de proyecto
- ‚úÖ Repositorio GitHub: `https://github.com/Cristian181199/retailflux-webscraper`

## üóÑÔ∏è Variables de Entorno Requeridas

### En Dokploy UI (Configuraci√≥n del Servicio)

#### **Base de Datos (si no est√°n a nivel de proyecto)**
```bash
POSTGRES_HOST=nombre_del_servicio_postgres
POSTGRES_PORT=5432
POSTGRES_DB=products_db
POSTGRES_USER=tu_usuario
POSTGRES_PASSWORD=tu_password
```

#### **Configuraci√≥n del Scraper**
```bash
# Modo de ejecuci√≥n
SCRAPER_MODE=wait

# Configuraci√≥n de scraping
SCRAPER_CONCURRENT_REQUESTS=2
SCRAPER_DOWNLOAD_DELAY=2.0
SCRAPER_USER_AGENT=ModernEdekaScraper/1.0 (+cristian181199@gmail.com)

# Rate limiting
ENABLE_RATE_LIMITING=true
REQUESTS_PER_MINUTE=30

# Base de datos
SQLALCHEMY_ECHO=false
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20

# Variables de aplicaci√≥n
APP_ENV=production
PYTHONPATH=/app/src
```

#### **AI Features (Opcional)**
```bash
OPENAI_API_KEY=tu_openai_api_key_aqui
ENABLE_AI_FEATURES=true
GENERATE_EMBEDDINGS=true
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536
```

## üê≥ Configuraci√≥n del Servicio en Dokploy

### 1. Crear Nuevo Servicio

1. **Ir a tu proyecto en Dokploy**
2. **Crear nuevo servicio** ‚Üí "Application"
3. **Configurar origen**:
   - **Source Type**: GitHub
   - **Repository**: `Cristian181199/retailflux-webscraper`
   - **Branch**: `main`

### 2. Configuraci√≥n Docker

```yaml
# Dokploy detectar√° autom√°ticamente el Dockerfile
# El Dockerfile ya est√° optimizado para producci√≥n
```

### 3. Configurar Variables de Entorno

En la secci√≥n **Environment Variables** del servicio, agregar todas las variables listadas arriba.

### 4. Configuraci√≥n de Red

- **Port Mapping**: No necesario (servicio de background)
- **Internal Network**: Conectar con la red donde est√° tu base de datos

### 5. Configuraci√≥n de Recursos

```yaml
# Recursos recomendados
CPU: 0.5 - 1.0 vCPU
Memory: 512MB - 1GB RAM
```

## ‚öôÔ∏è Despliegue del Servicio

### 1. Primera Ejecuci√≥n

```bash
# El servicio se desplegar√° en modo "wait" por defecto
# Esto mantiene el contenedor corriendo esperando comandos
```

### 2. Verificar el Deployment

1. **Check Logs**: Deber√≠as ver:
   ```
   üï∑Ô∏è RetailFlux Scraper
   ======================
   ‚è≥ Waiting for database connection...
   ‚úÖ Database connection established.
   ‚è∏Ô∏è  Scraper in WAIT mode. Ready for scheduled execution via Dokploy.
   ```

2. **Verificar Variables**: En los logs deber√≠as ver que la base de datos se conecta correctamente.

## üìÖ Configurar Schedule (S√°bados 03:00)

### M√©todo 1: Dokploy Native Schedule

1. **Ir a la configuraci√≥n del servicio**
2. **Buscar secci√≥n "Scheduled Jobs" o "Cron Jobs"**
3. **Crear nuevo job**:
   ```bash
   # Cron Expression: S√°bados a las 03:00
   0 3 * * 6
   
   # Comando
   docker exec [container_name] /usr/local/bin/entrypoint.sh run
   ```

### M√©todo 2: Schedule Manual (Si Dokploy no tiene scheduling nativo)

Crear un servicio adicional con un contenedor cron:

1. **Crear archivo `docker-compose.scheduler.yml`**:
   ```yaml
   version: '3.8'
   services:
     scraper-scheduler:
       image: alpine/cron:latest
       volumes:
         - /var/run/docker.sock:/var/run/docker.sock:ro
       environment:
         - DOCKER_HOST=unix:///var/run/docker.sock
         - SCRAPER_CONTAINER_NAME=retailflux-scraper
       command: |
         sh -c 'echo "0 3 * * 6 docker exec $SCRAPER_CONTAINER_NAME /usr/local/bin/entrypoint.sh run" | crontab - && crond -f'
   ```

### M√©todo 3: Ejecutar Manualmente

Si prefieres controlar la ejecuci√≥n manualmente:

```bash
# Ejecutar inmediatamente desde Dokploy Terminal
docker exec [container_name] /usr/local/bin/entrypoint.sh run

# O cambiar el modo del servicio
# Actualizar variable SCRAPER_MODE=run y redeploy
```

## üîç Monitoreo y Logs

### Ver Logs de Ejecuci√≥n

```bash
# En Dokploy, ir a la secci√≥n de Logs del servicio
# Filtrar por timestamp para ver ejecuciones espec√≠ficas
```

### Logs T√≠picos de Ejecuci√≥n Exitosa

```
üöÄ Starting scraper execution...
üìÑ Processing main sitemap: https://www.edeka24.de/sitemaps/sitemap.xml
üîó Found X sitemaps in index
üß™ Dev mode: Limited to Y product sitemaps
‚úÖ Scraped: Product Name - 5.29 EUR
=== DATABASE PIPELINE STATS ===
Items saved: X
Items updated: Y
‚úÖ Scraping completed successfully!
```

## üõ†Ô∏è Troubleshooting

### Error de Conexi√≥n a Base de Datos

1. **Verificar variables de entorno**
2. **Comprobar que el servicio de BD est√© corriendo**
3. **Revisar conectividad de red entre servicios**

```bash
# Test de conexi√≥n manual
docker exec [container_name] pg_isready -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER
```

### Error de Memoria

```bash
# Aumentar l√≠mites de memoria en la configuraci√≥n del servicio
# Recomendado: 1GB RAM m√≠nimo para scraping completo
```

### Error de Rate Limiting

```bash
# Ajustar variables:
SCRAPER_DOWNLOAD_DELAY=3.0
REQUESTS_PER_MINUTE=20
```

## üìä Optimizaci√≥n para Producci√≥n

### Variables Recomendadas para Producci√≥n

```bash
# Modo producci√≥n (no dev limits)
APP_ENV=production

# Logging optimizado
SQLALCHEMY_ECHO=false

# Scraping responsable
SCRAPER_DOWNLOAD_DELAY=2.0
SCRAPER_CONCURRENT_REQUESTS=2
ROBOTSTXT_OBEY=true

# Pool de conexiones optimizado
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
```

### Verificaci√≥n Final

1. ‚úÖ **Servicio desplegado** y corriendo en modo wait
2. ‚úÖ **Variables de entorno** configuradas
3. ‚úÖ **Conexi√≥n a BD** verificada
4. ‚úÖ **Schedule configurado** para s√°bados 03:00
5. ‚úÖ **Logs monitoreados** para verificar ejecuciones

## üéØ Comandos √ötiles de Dokploy

```bash
# Ver logs en tiempo real
dokploy logs follow [service_name]

# Restart del servicio
dokploy service restart [service_name]

# Ejecutar comando en contenedor
dokploy exec [service_name] [command]
```

¬°Tu RetailFlux WebScraper estar√° listo para scrapear autom√°ticamente cada s√°bado a las 03:00! üï∑Ô∏è‚ú®